{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e28e838-3ba1-4de0-b5bc-35adf907c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob as gb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import zipfile\n",
    "import warnings\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01a4422f-b37c-4411-9b66-6453ea4a10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_audio = '/Users/anushkaanand/Desktop/venv/cleansound/ravdess_rewritten_8k'\n",
    "noisy_audio = '/Users/anushkaanand/Desktop/venv/finalsound'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "292f5cfe-07d0-4643-b5f2-3be025a3d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC Features Shape: (5000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "TRACK_DURATION = 3 # measured in seconds\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "num_mfcc = 128\n",
    "n_fft = 2048\n",
    "hop_length = 615\n",
    "num_segments = 5\n",
    "\n",
    "mfcc_features_mixed_sound = []\n",
    "\n",
    "for filename in os.listdir(noisy_audio):\n",
    "    file_path = os.path.join(noisy_audio, filename)\n",
    "    y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "   \n",
    "    for d in range(num_segments):\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=SAMPLE_RATE, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        mfcc_features_mixed_sound.append(mfcc.T)\n",
    "        \n",
    "mfcc_features_mixed_sound = np.array(mfcc_features_mixed_sound)\n",
    "print(f'MFCC Features Shape: {mfcc_features_mixed_sound.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917deb1c-7921-4460-b1bf-65157715f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "TRACK_DURATION = 3 # measured in seconds\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "num_mfcc = 128\n",
    "n_fft = 2048\n",
    "hop_length = 615\n",
    "num_segments = 5\n",
    "\n",
    "mfcc_features_clean_audio = []\n",
    "\n",
    "for filename in os.listdir(clean_audio):\n",
    "    file_path = os.path.join(clean_audio, filename)\n",
    "    y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "    # samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    # num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "\n",
    "    for d in range(num_segments):\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=SAMPLE_RATE, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        mfcc_features_clean_audio.append(mfcc.T)\n",
    "        \n",
    "mfcc_features_clean_audio = np.array(mfcc_features_clean_audio)\n",
    "print(f'MFCC Features Shape: {mfcc_features_clean_audio.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "850cfc3a-0f25-4395-a034-4b40529bc2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4500, 26, 128)\n",
      "y_train shape: (4500, 26, 128)\n"
     ]
    }
   ],
   "source": [
    "min_samples = min(mfcc_features_mixed_sound.shape[0], mfcc_features_clean_audio.shape[0])\n",
    "\n",
    "# Trim both arrays to the smaller size\n",
    "noisy_mfcc_reshaped_trimmed = mfcc_features_mixed_sound[:min_samples]\n",
    "clean_mfcc_reshaped_trimmed = mfcc_features_clean_audio[:min_samples]\n",
    "\n",
    "# Now perform the train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    noisy_mfcc_reshaped_trimmed, clean_mfcc_reshaped_trimmed, test_size=0.1\n",
    ")\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96886a96-ff3d-46fb-bc37-97ded3379cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Activation, MaxPool2D, Concatenate\n",
    "def conv_block (input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return (x)\n",
    "\n",
    "def max_pooling (input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = MaxPool2D((2,2))(x)\n",
    "\n",
    "    return (x,p)\n",
    "\n",
    "def upscaling (input, num_filters, skip_features):\n",
    "    x = Conv2DTranspose(num_filters, (2,2), strides = 2, padding = \"same\")(input)\n",
    "    print(f\"Shape after Conv2DTranspose: {x.shape}\")\n",
    "    print(f\"Shape of skip_features: {skip_features.shape}\")\n",
    "    x = concatenate([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return (x)\n",
    "\n",
    "def U_NET (input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # s1, p1 = max_pooling(inputs, 64)\n",
    "    # print(s1.shape)\n",
    "    # print(p1.shape)\n",
    "    s2, p2 = max_pooling(inputs, 128)\n",
    "    s3, p3 = max_pooling(p2, 256)\n",
    "    s4 ,p4 = max_pooling(p3, 512)\n",
    "\n",
    "    b1 = conv_block(p4, 1024)\n",
    "\n",
    "    u4 = upscaling(b1, 512, s4)\n",
    "    u3 = upscaling(u4, 256, s3)\n",
    "    u2 = upscaling(u3, 128, s2)\n",
    "    print(u2.shape)\n",
    "    # u1 = upscaling(u2, 64, s1)\n",
    "    \n",
    "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(u2)\n",
    "    print(outputs.shape)\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51bae44e-ea7f-40e9-b32c-d59bdfb34fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.shape)\n",
    "# x_train = x_train[..., np.newaxis]\n",
    "# print(x_train.shape)\n",
    "# x_test = x_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b69e3d0-daaa-4b2b-8195-4e2fa69b0a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 26, 128)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "\n",
    "IMG_HEIGHT = x_train.shape[1]\n",
    "IMG_WIDTH  = x_train.shape[2]\n",
    "IMG_CHANNELS = 1\n",
    "\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1adc6a2d-fd8e-4a6e-bdb1-419ecf631aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2DTranspose, Cropping2D, ZeroPadding2D, concatenate\n",
    "\n",
    "def upscaling(input, num_filters, skip_features):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding='same')(input)\n",
    "    print(f\"Shape after Conv2DTranspose: {x.shape}\")\n",
    "    print(f\"Shape of skip_features: {skip_features.shape}\")\n",
    "\n",
    "    # Adjust shapes if necessary\n",
    "    # Example: Padding or Cropping\n",
    "    if x.shape[1] > skip_features.shape[1]:\n",
    "        x = Cropping2D(((x.shape[1] - skip_features.shape[1]) // 2, 0))(x)\n",
    "    elif x.shape[1] < skip_features.shape[1]:\n",
    "        x = ZeroPadding2D(((skip_features.shape[1] - x.shape[1]) // 2, 0))(x)\n",
    "\n",
    "    # Repeat for other dimensions if necessary\n",
    "\n",
    "    x = concatenate([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "433ba8ea-891c-456e-901b-9007ccde82d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Conv2DTranspose: (None, 6, 32, 512)\n",
      "Shape of skip_features: (None, 6, 32, 512)\n",
      "Shape after Conv2DTranspose: (None, 12, 64, 256)\n",
      "Shape of skip_features: (None, 13, 64, 256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 12, 64, 256), (None, 13, 64, 256)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mU_NET\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[0;32mIn[8], line 43\u001b[0m, in \u001b[0;36mU_NET\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m     40\u001b[0m b1 \u001b[38;5;241m=\u001b[39m conv_block(p4, \u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m     42\u001b[0m u4 \u001b[38;5;241m=\u001b[39m upscaling(b1, \u001b[38;5;241m512\u001b[39m, s4)\n\u001b[0;32m---> 43\u001b[0m u3 \u001b[38;5;241m=\u001b[39m \u001b[43mupscaling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m u2 \u001b[38;5;241m=\u001b[39m upscaling(u3, \u001b[38;5;241m128\u001b[39m, s2)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(u2\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36mupscaling\u001b[0;34m(input, num_filters, skip_features)\u001b[0m\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m ZeroPadding2D(((skip_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))(x)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Repeat for other dimensions if necessary\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m conv_block(x, num_filters)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/merging/concatenate.py:172\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.layers.concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcatenate\u001b[39m(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Functional interface to the `Concatenate` layer.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m        A tensor, the concatenation of the inputs alongside axis `axis`.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mConcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/merging/concatenate.py:97\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     91\u001b[0m         unique_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m     92\u001b[0m             shape[axis]\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m shape_set\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m shape[axis] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dims) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 97\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 12, 64, 256), (None, 13, 64, 256)]"
     ]
    }
   ],
   "source": [
    "model = U_NET(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate= 1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d5f2990-55e7-4b14-8406-6c1b20bcf2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Assuming each sample is a single row with 13 MFCC features\n",
    "# # Reshape x_train to (num_samples, 13, 1, 1) for a U-Net with a single channel\n",
    "# x_train_reshaped = x_train.reshape((x_train.shape[0], 13, 1, 1))\n",
    "# x_test_reshaped = x_test.reshape((x_test.shape[0], 13, 1, 1))\n",
    "\n",
    "# # Also ensure y_train and y_test are reshaped accordingly if needed\n",
    "# # Example: If y_train is binary classification, it could be reshaped to (num_samples, 1)\n",
    "# # print(y_train.shape)\n",
    "# # y_train_reshaped = y_train.reshape((y_train.shape[0], 1))\n",
    "# # y_test_reshaped = y_test.reshape((y_test.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba9223-cb6b-4174-9180-7acea9c84355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m 23/113\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:12:48\u001b[0m 89s/step - accuracy: 4.9450e-05 - loss: -259.3033"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7f954-c2db-401d-ae36-b517fa861bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
